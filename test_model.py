#!/usr/bin/env python3
# test_model.py - ìì²´ êµ¬í˜„ CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent / "src"))

import torch
from src.model import CLIPModel
from src.text_encoder import TextTransformer

def test_clip_model():
    print("=== ìì²´ êµ¬í˜„ CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # 1. ëª¨ë¸ ìƒì„±
        print("1. ëª¨ë¸ ìƒì„± ì¤‘...")
        model = CLIPModel()
        print(f"   âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(model).__name__}")
        
        # 2. í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
        print("2. í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸...")
        tokenizer = TextTransformer()
        texts = ['a photo of a cat', 'a photo of a dog']
        tokens = tokenizer.tokenize(texts)
        print(f"   âœ… í† í° í¬ê¸°: {tokens.shape}")
        print(f"   âœ… í† í° ì˜ˆì‹œ: {tokens[0][:10].tolist()}")
        
        # 3. í…ìŠ¤íŠ¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸
        print("3. í…ìŠ¤íŠ¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸...")
        with torch.no_grad():
            text_embeds = model.encode_text(tokens)
            print(f"   âœ… í…ìŠ¤íŠ¸ ì„ë² ë”© í¬ê¸°: {text_embeds.shape}")
            print(f"   âœ… ì„ë² ë”© norm: {text_embeds.norm(dim=-1)}")
        
        # 4. ì´ë¯¸ì§€ ì„ë² ë”© í…ŒìŠ¤íŠ¸ (ë”ë¯¸ ë°ì´í„°)
        print("4. ì´ë¯¸ì§€ ì„ë² ë”© í…ŒìŠ¤íŠ¸...")
        dummy_images = torch.randn(2, 3, 224, 224)
        with torch.no_grad():
            image_embeds = model.encode_image(dummy_images)
            print(f"   âœ… ì´ë¯¸ì§€ ì„ë² ë”© í¬ê¸°: {image_embeds.shape}")
            print(f"   âœ… ì„ë² ë”© norm: {image_embeds.norm(dim=-1)}")
        
        # 5. ìœ ì‚¬ë„ ê³„ì‚°
        print("5. ìœ ì‚¬ë„ ê³„ì‚°...")
        similarity = (image_embeds @ text_embeds.t())
        print(f"   âœ… ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {similarity.shape}")
        print(f"   âœ… ìœ ì‚¬ë„ ê°’: {similarity}")
        print(f"   âœ… Temperature: {model.temperature.item():.4f}")
        
        # 6. ì „ì²´ forward í…ŒìŠ¤íŠ¸
        print("6. ì „ì²´ forward í…ŒìŠ¤íŠ¸...")
        with torch.no_grad():
            output = model(dummy_images, tokens)
            print(f"   âœ… Forward output keys: {list(output.keys())}")
            print(f"   âœ… logits_per_image í¬ê¸°: {output['logits_per_image'].shape}")
            
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ìì²´ êµ¬í˜„ CLIP ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_clip_model()
    sys.exit(0 if success else 1) 