{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import clip  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 설정 함수\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "def clip_contrastive_loss(logits_per_image: torch.Tensor, logits_per_text: torch.Tensor) -> torch.Tensor:\n",
    "    b = logits_per_image.size(0)\n",
    "    target = torch.arange(b, device=logits_per_image.device)\n",
    "    return 0.5 * (F.cross_entropy(logits_per_image, target) +\n",
    "                  F.cross_entropy(logits_per_text,  target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPBackbone(nn.Module):\n",
    "    def __init__(self, model_name: str = \"ViT-B/32\", device: Optional[str] = None, finetune: bool = False):\n",
    "        super().__init__()\n",
    "        dev = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # clip.load은 모델과 preprocess(transform)를 함께 반환\n",
    "        model, preprocess = clip.load(model_name, device=dev, jit=False)\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess  # PIL->Tensor 변환(논문 통계/해상도 포함)\n",
    "\n",
    "        if not finetune:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    @property\n",
    "    def context_length(self) -> int:\n",
    "        # CLIP 기본 컨텍스트 길이: 77\n",
    "        return self.model.context_length\n",
    "\n",
    "    @property\n",
    "    def logit_scale(self) -> torch.Tensor:\n",
    "        return self.model.logit_scale\n",
    "\n",
    "    @property\n",
    "    def temperature(self) -> torch.Tensor:\n",
    "        return self.model.logit_scale.exp()\n",
    "\n",
    "    def tokenize(self, texts: List[str]) -> torch.LongTensor:\n",
    "        # 공식 BPE 토크나이저 (77 토큰, 필요시 truncate)\n",
    "        return clip.tokenize(texts, context_length=self.context_length, truncate=True)\n",
    "\n",
    "    def encode_image(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        # 이미 L2 normalize된 임베딩 반환\n",
    "        return self.model.encode_image(images)\n",
    "\n",
    "    def encode_text(self, text_tokens: torch.LongTensor) -> torch.Tensor:\n",
    "        return self.model.encode_text(text_tokens)\n",
    "\n",
    "    def forward(self, images: torch.Tensor, text_tokens: torch.LongTensor):\n",
    "        img_emb = self.model.encode_image(images)          # (B, D), L2 norm\n",
    "        txt_emb = self.model.encode_text(text_tokens)      # (B, D), L2 norm\n",
    "        # 논문식 로짓: s * (I @ T^T)\n",
    "        logits = self.temperature * img_emb @ txt_emb.t()\n",
    "        return {\n",
    "            \"image_embeds\": img_emb,\n",
    "            \"text_embeds\": txt_emb,\n",
    "            \"logits_per_image\": logits,\n",
    "            \"logits_per_text\": logits.t(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "class ImageTextCsv(Dataset):\n",
    "    def __init__(self, csv_path: str, img_root: str, preprocess):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = Path(img_root)\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, object]:\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(self.root / row[\"image_path\"]).convert(\"RGB\")\n",
    "        return {\"image\": self.preprocess(img), \"text\": str(row[\"caption\"])}\n",
    "\n",
    "def build_collate_clip(tokenize, context_length: int = 77):\n",
    "    def _fn(batch: List[dict]):\n",
    "        images = torch.stack([b[\"image\"] for b in batch])\n",
    "        texts  = [b[\"text\"] for b in batch]\n",
    "        tokens = tokenize(texts)  # (B, 77)\n",
    "        return {\"images\": images, \"text_tokens\": tokens, \"raw_texts\": texts}\n",
    "    return _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLightning(pl.LightningModule):\n",
    "    def __init__(self, model_name: str = \"ViT-B/32\", lr: float = 1e-4, weight_decay: float = 0.01,\n",
    "                 finetune: bool = False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.backbone = CLIPBackbone(model_name=model_name, finetune=finetune)\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return self.backbone.temperature\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        out = self.backbone(images=batch[\"images\"], text_tokens=batch[\"text_tokens\"])\n",
    "        loss = clip_contrastive_loss(out[\"logits_per_image\"], out[\"logits_per_text\"])\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/logit_scale\", self.temperature, on_step=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        out = self.backbone(images=batch[\"images\"], text_tokens=batch[\"text_tokens\"])\n",
    "        loss = clip_contrastive_loss(out[\"logits_per_image\"], out[\"logits_per_text\"])\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        # 공식 구현 관행: logit_scale 상한 제한(안정화)\n",
    "        with torch.no_grad():\n",
    "            self.backbone.logit_scale.clamp_(max=math.log(100.0))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 인코더를 finetune=False로 두면 학습되는 것은 logit_scale뿐. finetune=True면 전체 학습.\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay, betas=(0.9, 0.98))\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.trainer.max_steps or 1000)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_csv: str, img_root: str, val_csv: str = None, batch_size: int = 128,\n",
    "                 num_workers: int = 4, model_name: str = \"ViT-B/32\"):\n",
    "        super().__init__()\n",
    "        # preprocess/tokenize는 모델에서 가져오되, 여기서는 별도 로딩 방지를 위해 한 번만 생성\n",
    "        bb = CLIPBackbone(model_name=model_name, finetune=False)  # 데이터 준비 용도이므로 freeze OK\n",
    "        self.preprocess = bb.preprocess\n",
    "        self.tokenize = bb.tokenize\n",
    "        self.context_length = bb.context_length\n",
    "\n",
    "        self.train_csv, self.val_csv = train_csv, val_csv\n",
    "        self.img_root = img_root\n",
    "        self.batch_size, self.num_workers = batch_size, num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds_train = ImageTextCsv(self.train_csv, self.img_root, self.preprocess)\n",
    "        self.ds_val   = ImageTextCsv(self.val_csv,   self.img_root, self.preprocess) if self.val_csv else None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=True,\n",
    "                          persistent_workers=self.num_workers>0, prefetch_factor=2 if self.num_workers>0 else None,\n",
    "                          drop_last=True, collate_fn=build_collate_clip(self.tokenize, self.context_length))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.ds_val is None: return None\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True,\n",
    "                          persistent_workers=self.num_workers>0, prefetch_factor=2 if self.num_workers>0 else None,\n",
    "                          drop_last=False, collate_fn=build_collate_clip(self.tokenize, self.context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 하이퍼파라미터 설정\n",
    "model_name = \"ViT-B/32\"  # \"ViT-B/32\",\"ViT-B/16\",\"ViT-L/14\",\"RN50\" 등\n",
    "train_csv = \"data/train.csv\"\n",
    "val_csv = None  # 검증 데이터가 있으면 경로 지정\n",
    "img_root = \"data/images\"\n",
    "batch_size = 128\n",
    "num_workers = 0  # 윈도우에서는 0 권장\n",
    "epochs = 3\n",
    "lr = 1e-4\n",
    "weight_decay = 0.01\n",
    "finetune = False  # True면 전체 미세조정, False면 logit_scale만 학습\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 디렉토리를 C:\\python\\clip-demo로 변경했습니다\n",
      "현재 작업 디렉토리: C:\\python\\clip-demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type         | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | backbone | CLIPBackbone | 151 M  | train\n",
      "--------------------------------------------------\n",
      "0         Trainable params\n",
      "151 M     Non-trainable params\n",
      "151 M     Total params\n",
      "605.109   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "227       Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    }
   ],
   "source": [
    "project_root = Path(\"C:/python/clip-demo\")  # 실제 프로젝트 경로\n",
    "if project_root.exists():\n",
    "    os.chdir(project_root)\n",
    "    print(f\"작업 디렉토리를 {project_root}로 변경했습니다\")\n",
    "else:\n",
    "    print(\"프로젝트 디렉토리를 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "# 시드 설정\n",
    "set_seed(seed)\n",
    "\n",
    "# 데이터 모듈 생성\n",
    "dm = CLIPDataModule(train_csv, img_root, val_csv,\n",
    "                    batch_size=batch_size, num_workers=num_workers,\n",
    "                    model_name=model_name)\n",
    "\n",
    "# 모델 생성\n",
    "model = CLIPLightning(model_name=model_name, lr=lr, weight_decay=weight_decay, finetune=finetune)\n",
    "\n",
    "# 트레이너 설정\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    precision=\"32\",  # FP32로 변경하여 mixed precision 문제 해결\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    num_sanity_val_steps=0,  # validation sanity check 비활성화\n",
    "    limit_val_batches=0,  # validation 배치 수를 0으로 설정\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 위치한 디바이스: cpu\n",
      "텍스트 토큰 디바이스: cpu\n",
      "텍스트 임베딩 크기: torch.Size([2, 512])\n",
      "텍스트 임베딩 디바이스: cpu\n",
      "임베딩 norm: tensor([10.4766, 10.6953], dtype=torch.float16)\n",
      "텍스트 간 코사인 유사도: 104.3125\n",
      "Temperature: 100.0000\n",
      "Temperature 적용된 유사도: 10431.2508\n",
      "\n",
      "텍스트 간 유사도 매트릭스 (코사인 유사도):\n",
      "a photo of a ca vs a photo of a ca: 109.6875\n",
      "a photo of a ca vs a photo of a do: 104.3125\n",
      "a photo of a ca vs a photo of a ca: 95.2500\n",
      "a photo of a ca vs a photo of a tr: 81.3125\n",
      "a photo of a do vs a photo of a ca: 104.3125\n",
      "a photo of a do vs a photo of a do: 114.4375\n",
      "a photo of a do vs a photo of a ca: 98.2500\n",
      "a photo of a do vs a photo of a tr: 83.3750\n",
      "a photo of a ca vs a photo of a ca: 95.2500\n",
      "a photo of a ca vs a photo of a do: 98.2500\n",
      "a photo of a ca vs a photo of a ca: 109.4375\n",
      "a photo of a ca vs a photo of a tr: 81.9375\n",
      "a photo of a tr vs a photo of a ca: 81.3125\n",
      "a photo of a tr vs a photo of a do: 83.3750\n",
      "a photo of a tr vs a photo of a ca: 81.9375\n",
      "a photo of a tr vs a photo of a tr: 89.6875\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로 간단한 테스트\n",
    "model.eval()\n",
    "\n",
    "# 디바이스 확인 및 설정\n",
    "device = next(model.parameters()).device\n",
    "print(f\"모델이 위치한 디바이스: {device}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 텍스트 임베딩 생성 (정규화된 임베딩 직접 사용)\n",
    "    texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "    text_tokens = model.backbone.tokenize(texts)\n",
    "    \n",
    "    # 토큰을 모델과 같은 디바이스로 이동\n",
    "    text_tokens = text_tokens.to(device)\n",
    "    print(f\"텍스트 토큰 디바이스: {text_tokens.device}\")\n",
    "    \n",
    "    # 정규화된 텍스트 임베딩 얻기 (temperature 곱하기 전)\n",
    "    text_embeds = model.backbone.encode_text(text_tokens)\n",
    "    \n",
    "    print(f\"텍스트 임베딩 크기: {text_embeds.shape}\")\n",
    "    print(f\"텍스트 임베딩 디바이스: {text_embeds.device}\")\n",
    "    print(f\"임베딩 norm: {text_embeds.norm(dim=-1)}\")  # L2 정규화 확인\n",
    "    \n",
    "    # 코사인 유사도 계산 (정규화된 벡터 간의 내적)\n",
    "    cosine_similarity = (text_embeds[0] @ text_embeds[1]).item()\n",
    "    print(f\"텍스트 간 코사인 유사도: {cosine_similarity:.4f}\")\n",
    "    \n",
    "    # temperature가 적용된 유사도 (CLIP에서 실제 사용되는 값)\n",
    "    temperature = model.backbone.temperature.item()\n",
    "    scaled_similarity = temperature * cosine_similarity\n",
    "    print(f\"Temperature: {temperature:.4f}\")\n",
    "    print(f\"Temperature 적용된 유사도: {scaled_similarity:.4f}\")\n",
    "    \n",
    "    # 추가 테스트: 다른 텍스트들과의 유사도\n",
    "    more_texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a tree\"]\n",
    "    more_tokens = model.backbone.tokenize(more_texts)\n",
    "    more_tokens = more_tokens.to(device)  # 디바이스 맞추기\n",
    "    \n",
    "    more_embeds = model.backbone.encode_text(more_tokens)\n",
    "    similarity_matrix = more_embeds @ more_embeds.t()\n",
    "    \n",
    "    print(\"\\n텍스트 간 유사도 매트릭스 (코사인 유사도):\")\n",
    "    for i, text1 in enumerate(more_texts):\n",
    "        for j, text2 in enumerate(more_texts):\n",
    "            sim = similarity_matrix[i, j].item()\n",
    "            print(f\"{text1[:15]:15} vs {text2[:15]:15}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
