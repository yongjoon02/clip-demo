{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.10.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'f:/clip-demo/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_contrastive_loss(logits_per_image: torch.Tensor, logits_per_text: torch.Tensor) -> torch.Tensor:\n",
    "    b = logits_per_image.size(0)\n",
    "    target = torch.arange(b, device=logits_per_image.device)\n",
    "    return 0.5 * (F.cross_entropy(logits_per_image, target) +\n",
    "                  F.cross_entropy(logits_per_text,  target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=32, embed_dim=512, num_layers=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=embed_dim*4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = self.ln_final(x[:, 0])\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return F.normalize(x, p=2, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embed_dim=512, max_len=77, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim*4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def tokenize(self, texts: List[str]) -> torch.LongTensor:\n",
    "        tokens = []\n",
    "        for text in texts:\n",
    "            words = text.lower().replace('.', '').replace(',', '').split()\n",
    "            token_ids = [1]\n",
    "            for word in words[:self.max_len-2]:\n",
    "                token_id = (hash(word) % (self.vocab_size - 100)) + 100\n",
    "                token_ids.append(token_id)\n",
    "            token_ids.append(2)\n",
    "            \n",
    "            while len(token_ids) < self.max_len:\n",
    "                token_ids.append(0)\n",
    "                \n",
    "            tokens.append(token_ids[:self.max_len])\n",
    "        \n",
    "        return torch.LongTensor(tokens)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        x = self.token_embed(token_ids)\n",
    "        \n",
    "        x = x + self.pos_embed[:, :x.size(1)]\n",
    "        \n",
    "        mask = (token_ids == 0)\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        seq_lens = (token_ids != 0).sum(dim=1) - 1\n",
    "        batch_idx = torch.arange(x.size(0))\n",
    "        x = x[batch_idx, seq_lens]\n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return F.normalize(x, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIP(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=32, embed_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_encoder = SimpleViT(image_size, patch_size, embed_dim)\n",
    "        self.text_encoder = SimpleTextEncoder(embed_dim=embed_dim)\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return self.logit_scale.exp()\n",
    "    \n",
    "    def encode_image(self, images):\n",
    "        return self.image_encoder(images)\n",
    "    \n",
    "    def encode_text(self, text_tokens):\n",
    "        return self.text_encoder(text_tokens)\n",
    "    \n",
    "    def forward(self, images, text_tokens):\n",
    "        image_embeds = self.encode_image(images)\n",
    "        text_embeds = self.encode_text(text_tokens)\n",
    "        \n",
    "        logits = self.temperature * image_embeds @ text_embeds.t()\n",
    "        \n",
    "        return {\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"text_embeds\": text_embeds, \n",
    "            \"logits_per_image\": logits,\n",
    "            \"logits_per_text\": logits.t()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "class ImageTextCsv(Dataset):\n",
    "    def __init__(self, csv_path: str, img_root: str, preprocess):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = Path(img_root)\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, object]:\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(self.root / row[\"image_path\"]).convert(\"RGB\")\n",
    "        return {\"image\": self.preprocess(img), \"text\": str(row[\"caption\"])}\n",
    "\n",
    "def build_collate_clip(tokenize_fn):\n",
    "    def _fn(batch: List[dict]):\n",
    "        images = torch.stack([b[\"image\"] for b in batch])\n",
    "        texts  = [b[\"text\"] for b in batch]\n",
    "        tokens = tokenize_fn(texts)\n",
    "        return {\"images\": images, \"text_tokens\": tokens, \"raw_texts\": texts}\n",
    "    return _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIPLightning(pl.LightningModule):\n",
    "    def __init__(self, embed_dim=512, lr: float = 1e-4, weight_decay: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SimpleCLIP(embed_dim=embed_dim)\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return self.model.temperature\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        out = self.model(images=batch[\"images\"], text_tokens=batch[\"text_tokens\"])\n",
    "        loss = clip_contrastive_loss(out[\"logits_per_image\"], out[\"logits_per_text\"])\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/logit_scale\", self.temperature, on_step=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        out = self.model(images=batch[\"images\"], text_tokens=batch[\"text_tokens\"])\n",
    "        loss = clip_contrastive_loss(out[\"logits_per_image\"], out[\"logits_per_text\"])\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        with torch.no_grad():\n",
    "            self.model.logit_scale.clamp_(max=math.log(100.0))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay, betas=(0.9, 0.98))\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.trainer.max_steps or 1000)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIPDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_csv: str, img_root: str, val_csv: str = None, batch_size: int = 128,\n",
    "                 num_workers: int = 4, embed_dim: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        clip_model = SimpleCLIP(embed_dim=embed_dim)\n",
    "        self.preprocess = clip_model.preprocess\n",
    "        self.tokenize = clip_model.text_encoder.tokenize\n",
    "\n",
    "        self.train_csv, self.val_csv = train_csv, val_csv\n",
    "        self.img_root = img_root\n",
    "        self.batch_size, self.num_workers = batch_size, num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds_train = ImageTextCsv(self.train_csv, self.img_root, self.preprocess)\n",
    "        self.ds_val   = ImageTextCsv(self.val_csv,   self.img_root, self.preprocess) if self.val_csv else None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=True,\n",
    "                          persistent_workers=self.num_workers>0, prefetch_factor=2 if self.num_workers>0 else None,\n",
    "                          drop_last=True, collate_fn=build_collate_clip(self.tokenize))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.ds_val is None: return None\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True,\n",
    "                          persistent_workers=self.num_workers>0, prefetch_factor=2 if self.num_workers>0 else None,\n",
    "                          drop_last=False, collate_fn=build_collate_clip(self.tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512 \n",
    "train_csv = \"data/train.csv\"\n",
    "val_csv = None \n",
    "img_root = \"data/images\"\n",
    "batch_size = 32 \n",
    "num_workers = 0 \n",
    "epochs = 3\n",
    "lr = 1e-3  \n",
    "weight_decay = 0.01\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 디렉토리를 C:\\python\\clip-demo로 변경했습니다\n",
      "현재 작업 디렉토리: C:\\python\\clip-demo\n",
      "모델 파라미터 수: 38,810,113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | SimpleCLIP | 38.8 M | train\n",
      "---------------------------------------------\n",
      "38.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "38.8 M    Total params\n",
      "155.240   Total estimated model params size (MB)\n",
      "113       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    }
   ],
   "source": [
    "project_root = Path(\"C:/python/clip-demo\")  \n",
    "if project_root.exists():\n",
    "    os.chdir(project_root)\n",
    "    print(f\"작업 디렉토리를 {project_root}로 변경했습니다\")\n",
    "else:\n",
    "    print(\"프로젝트 디렉토리를 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "dm = SimpleCLIPDataModule(train_csv, img_root, val_csv,\n",
    "                         batch_size=batch_size, num_workers=num_workers,\n",
    "                         embed_dim=embed_dim)\n",
    "\n",
    "\n",
    "model = SimpleCLIPLightning(embed_dim=embed_dim, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    precision=\"32\",\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    limit_val_batches=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 위치한 디바이스: cpu\n",
      "텍스트 토큰 shape: torch.Size([2, 77])\n",
      "텍스트 토큰 디바이스: cpu\n",
      "텍스트 임베딩 크기: torch.Size([2, 512])\n",
      "텍스트 임베딩 디바이스: cpu\n",
      "임베딩 norm: tensor([1.0000, 1.0000])\n",
      "텍스트 간 코사인 유사도: 0.9800\n",
      "Temperature: 14.2857\n",
      "Temperature 적용된 유사도: 13.9997\n",
      "\n",
      "텍스트 간 유사도 매트릭스 (코사인 유사도):\n",
      "a photo of a ca vs a photo of a ca: 1.0000\n",
      "a photo of a ca vs a photo of a do: 0.9800\n",
      "a photo of a ca vs a photo of a ca: 0.9801\n",
      "a photo of a ca vs a photo of a tr: 0.9798\n",
      "a photo of a do vs a photo of a ca: 0.9800\n",
      "a photo of a do vs a photo of a do: 1.0000\n",
      "a photo of a do vs a photo of a ca: 0.9852\n",
      "a photo of a do vs a photo of a tr: 0.9849\n",
      "a photo of a ca vs a photo of a ca: 0.9801\n",
      "a photo of a ca vs a photo of a do: 0.9852\n",
      "a photo of a ca vs a photo of a ca: 1.0000\n",
      "a photo of a ca vs a photo of a tr: 0.9870\n",
      "a photo of a tr vs a photo of a ca: 0.9798\n",
      "a photo of a tr vs a photo of a do: 0.9849\n",
      "a photo of a tr vs a photo of a ca: 0.9870\n",
      "a photo of a tr vs a photo of a tr: 1.0000\n",
      "\n",
      "이미지 임베딩 크기: torch.Size([2, 512])\n",
      "이미지 임베딩 norm: tensor([1.0000, 1.0000])\n",
      "\n",
      "이미지-텍스트 유사도 매트릭스:\n",
      "tensor([[0.0116, 0.0236],\n",
      "        [0.0195, 0.0320]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(f\"모델이 위치한 디바이스: {device}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "    text_tokens = model.model.text_encoder.tokenize(texts)\n",
    "    \n",
    "\n",
    "    text_tokens = text_tokens.to(device)\n",
    "    print(f\"텍스트 토큰 shape: {text_tokens.shape}\")\n",
    "    print(f\"텍스트 토큰 디바이스: {text_tokens.device}\")\n",
    "    \n",
    " \n",
    "    text_embeds = model.model.encode_text(text_tokens)\n",
    "    \n",
    "    print(f\"텍스트 임베딩 크기: {text_embeds.shape}\")\n",
    "    print(f\"텍스트 임베딩 디바이스: {text_embeds.device}\")\n",
    "    print(f\"임베딩 norm: {text_embeds.norm(dim=-1)}\")\n",
    "   \n",
    "    cosine_similarity = (text_embeds[0] @ text_embeds[1]).item()\n",
    "    print(f\"텍스트 간 코사인 유사도: {cosine_similarity:.4f}\")\n",
    "    \n",
    " \n",
    "    temperature = model.model.temperature.item()\n",
    "    scaled_similarity = temperature * cosine_similarity\n",
    "    print(f\"Temperature: {temperature:.4f}\")\n",
    "    print(f\"Temperature 적용된 유사도: {scaled_similarity:.4f}\")\n",
    "\n",
    "    more_texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a tree\"]\n",
    "    more_tokens = model.model.text_encoder.tokenize(more_texts)\n",
    "    more_tokens = more_tokens.to(device)\n",
    "    \n",
    "    more_embeds = model.model.encode_text(more_tokens)\n",
    "    similarity_matrix = more_embeds @ more_embeds.t()\n",
    "    \n",
    "    print(f\"\\n텍스트 간 유사도 매트릭스 (코사인 유사도):\")\n",
    "    for i, text1 in enumerate(more_texts):\n",
    "        for j, text2 in enumerate(more_texts):\n",
    "            sim = similarity_matrix[i, j].item()\n",
    "            print(f\"{text1[:15]:15} vs {text2[:15]:15}: {sim:.4f}\")\n",
    "    \n",
    "\n",
    "    dummy_images = torch.randn(2, 3, 224, 224).to(device)\n",
    "    image_embeds = model.model.encode_image(dummy_images)\n",
    "    print(f\"\\n이미지 임베딩 크기: {image_embeds.shape}\")\n",
    "    print(f\"이미지 임베딩 norm: {image_embeds.norm(dim=-1)}\")\n",
    "    \n",
    "\n",
    "    img_text_sim = image_embeds @ text_embeds.t()\n",
    "    print(f\"\\n이미지-텍스트 유사도 매트릭스:\")\n",
    "    print(img_text_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
