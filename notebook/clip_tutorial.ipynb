{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 설정 함수\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Contrastive Loss\n",
    "def clip_contrastive_loss(logits_per_image: torch.Tensor, logits_per_text: torch.Tensor) -> torch.Tensor:\n",
    "    b = logits_per_image.size(0)\n",
    "    target = torch.arange(b, device=logits_per_image.device)\n",
    "    return 0.5 * (F.cross_entropy(logits_per_image, target) +\n",
    "                  F.cross_entropy(logits_per_text,  target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 Vision Transformer (이미지 인코더)\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=32, embed_dim=512, num_layers=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # 패치 임베딩\n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # 위치 임베딩 (CLS 토큰 + 패치들)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Transformer 블록들\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=embed_dim*4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 최종 프로젝션\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # 패치로 나누기 (B, 3, H, W) -> (B, embed_dim, H/P, W/P) -> (B, N, embed_dim)\n",
    "        x = self.patch_embed(x)  # (B, embed_dim, 7, 7)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, 49, embed_dim)\n",
    "        \n",
    "        # CLS 토큰 추가\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, 50, embed_dim)\n",
    "        \n",
    "        # 위치 임베딩 추가\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # CLS 토큰만 사용\n",
    "        x = self.ln_final(x[:, 0])  # (B, embed_dim)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # L2 정규화\n",
    "        return F.normalize(x, p=2, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embed_dim=512, max_len=77, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # 토큰 임베딩\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim*4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 최종 프로젝션\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def tokenize(self, texts: List[str]) -> torch.LongTensor:\n",
    "        \"\"\"간단한 단어 기반 토크나이저\"\"\"\n",
    "        # 간단한 전처리\n",
    "        tokens = []\n",
    "        for text in texts:\n",
    "            # 소문자 변환, 특수문자 제거, 공백으로 분할\n",
    "            words = text.lower().replace('.', '').replace(',', '').split()\n",
    "            # 단어를 숫자로 변환 (해시 기반 - 실제로는 vocab 사전 필요)\n",
    "            token_ids = [1]  # SOS 토큰\n",
    "            for word in words[:self.max_len-2]:  # SOS, EOS 공간 확보\n",
    "                # 간단한 해시 기반 토큰화 (실제로는 BPE나 WordPiece 사용)\n",
    "                token_id = (hash(word) % (self.vocab_size - 100)) + 100  # 특수토큰 공간 확보\n",
    "                token_ids.append(token_id)\n",
    "            token_ids.append(2)  # EOS 토큰\n",
    "            \n",
    "            # 패딩\n",
    "            while len(token_ids) < self.max_len:\n",
    "                token_ids.append(0)  # PAD 토큰\n",
    "                \n",
    "            tokens.append(token_ids[:self.max_len])\n",
    "        \n",
    "        return torch.LongTensor(tokens)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        # 토큰 임베딩\n",
    "        x = self.token_embed(token_ids)  # (B, seq_len, embed_dim)\n",
    "        \n",
    "        # 위치 임베딩 추가\n",
    "        x = x + self.pos_embed[:, :x.size(1)]\n",
    "        \n",
    "        # 패딩 마스크 생성\n",
    "        mask = (token_ids == 0)  # PAD 토큰 위치\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # EOS 토큰 위치 찾기 (간단하게 마지막 non-pad 토큰 사용)\n",
    "        seq_lens = (token_ids != 0).sum(dim=1) - 1  # EOS 토큰 위치\n",
    "        batch_idx = torch.arange(x.size(0))\n",
    "        x = x[batch_idx, seq_lens]  # (B, embed_dim)\n",
    "        \n",
    "        # 최종 처리\n",
    "        x = self.ln_final(x)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # L2 정규화\n",
    "        return F.normalize(x, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIP(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=32, embed_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 이미지와 텍스트 인코더\n",
    "        self.image_encoder = SimpleViT(image_size, patch_size, embed_dim)\n",
    "        self.text_encoder = SimpleTextEncoder(embed_dim=embed_dim)\n",
    "        \n",
    "        # Temperature 파라미터 (학습 가능)\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))\n",
    "        \n",
    "        # 이미지 전처리\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return self.logit_scale.exp()\n",
    "    \n",
    "    def encode_image(self, images):\n",
    "        return self.image_encoder(images)\n",
    "    \n",
    "    def encode_text(self, text_tokens):\n",
    "        return self.text_encoder(text_tokens)\n",
    "    \n",
    "    def forward(self, images, text_tokens):\n",
    "        # 임베딩 추출\n",
    "        image_embeds = self.encode_image(images)  # (B, embed_dim)\n",
    "        text_embeds = self.encode_text(text_tokens)  # (B, embed_dim)\n",
    "        \n",
    "        # 유사도 계산 (temperature 적용)\n",
    "        logits = self.temperature * image_embeds @ text_embeds.t()\n",
    "        \n",
    "        return {\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"text_embeds\": text_embeds, \n",
    "            \"logits_per_image\": logits,\n",
    "            \"logits_per_text\": logits.t()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "class ImageTextCsv(Dataset):\n",
    "    def __init__(self, csv_path: str, img_root: str, preprocess):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = Path(img_root)\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, object]:\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(self.root / row[\"image_path\"]).convert(\"RGB\")\n",
    "        return {\"image\": self.preprocess(img), \"text\": str(row[\"caption\"])}\n",
    "\n",
    "def build_collate_clip(tokenize_fn):\n",
    "    def _fn(batch: List[dict]):\n",
    "        images = torch.stack([b[\"image\"] for b in batch])\n",
    "        texts  = [b[\"text\"] for b in batch]\n",
    "        tokens = tokenize_fn(texts)  # (B, 77)\n",
    "        return {\"images\": images, \"text_tokens\": tokens, \"raw_texts\": texts}\n",
    "    return _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIPLightning(pl.LightningModule):\n",
    "    def __init__(self, embed_dim=512, lr: float = 1e-4, weight_decay: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SimpleCLIP(embed_dim=embed_dim)\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return self.model.temperature\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        out = self.model(images=batch[\"images\"], text_tokens=batch[\"text_tokens\"])\n",
    "        loss = clip_contrastive_loss(out[\"logits_per_image\"], out[\"logits_per_text\"])\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/logit_scale\", self.temperature, on_step=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        out = self.model(images=batch[\"images\"], text_tokens=batch[\"text_tokens\"])\n",
    "        loss = clip_contrastive_loss(out[\"logits_per_image\"], out[\"logits_per_text\"])\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        # logit_scale 상한 제한 (안정화)\n",
    "        with torch.no_grad():\n",
    "            self.model.logit_scale.clamp_(max=math.log(100.0))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay, betas=(0.9, 0.98))\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.trainer.max_steps or 1000)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCLIPDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_csv: str, img_root: str, val_csv: str = None, batch_size: int = 128,\n",
    "                 num_workers: int = 4, embed_dim: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 간단한 CLIP 모델에서 전처리와 토크나이저 가져오기\n",
    "        clip_model = SimpleCLIP(embed_dim=embed_dim)\n",
    "        self.preprocess = clip_model.preprocess\n",
    "        self.tokenize = clip_model.text_encoder.tokenize\n",
    "\n",
    "        self.train_csv, self.val_csv = train_csv, val_csv\n",
    "        self.img_root = img_root\n",
    "        self.batch_size, self.num_workers = batch_size, num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds_train = ImageTextCsv(self.train_csv, self.img_root, self.preprocess)\n",
    "        self.ds_val   = ImageTextCsv(self.val_csv,   self.img_root, self.preprocess) if self.val_csv else None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=True,\n",
    "                          persistent_workers=self.num_workers>0, prefetch_factor=2 if self.num_workers>0 else None,\n",
    "                          drop_last=True, collate_fn=build_collate_clip(self.tokenize))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.ds_val is None: return None\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True,\n",
    "                          persistent_workers=self.num_workers>0, prefetch_factor=2 if self.num_workers>0 else None,\n",
    "                          drop_last=False, collate_fn=build_collate_clip(self.tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 하이퍼파라미터 설정\n",
    "embed_dim = 512  # 임베딩 차원\n",
    "train_csv = \"data/train.csv\"\n",
    "val_csv = None  # 검증 데이터가 있으면 경로 지정\n",
    "img_root = \"data/images\"\n",
    "batch_size = 32  # 작은 모델이므로 배치 크기 줄임\n",
    "num_workers = 0  # 윈도우에서는 0 권장\n",
    "epochs = 3\n",
    "lr = 1e-3  # 처음부터 학습하므로 학습률 높임\n",
    "weight_decay = 0.01\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 디렉토리를 C:\\python\\clip-demo로 변경했습니다\n",
      "현재 작업 디렉토리: C:\\python\\clip-demo\n",
      "모델 파라미터 수: 38,810,113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | SimpleCLIP | 38.8 M | train\n",
      "---------------------------------------------\n",
      "38.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "38.8 M    Total params\n",
      "155.240   Total estimated model params size (MB)\n",
      "113       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    }
   ],
   "source": [
    "project_root = Path(\"C:/python/clip-demo\")  # 실제 프로젝트 경로\n",
    "if project_root.exists():\n",
    "    os.chdir(project_root)\n",
    "    print(f\"작업 디렉토리를 {project_root}로 변경했습니다\")\n",
    "else:\n",
    "    print(\"프로젝트 디렉토리를 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "# 시드 설정\n",
    "set_seed(seed)\n",
    "\n",
    "# 데이터 모듈 생성\n",
    "dm = SimpleCLIPDataModule(train_csv, img_root, val_csv,\n",
    "                         batch_size=batch_size, num_workers=num_workers,\n",
    "                         embed_dim=embed_dim)\n",
    "\n",
    "# 모델 생성\n",
    "model = SimpleCLIPLightning(embed_dim=embed_dim, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 트레이너 설정\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    precision=\"32\",\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    limit_val_batches=0,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 위치한 디바이스: cpu\n",
      "텍스트 토큰 shape: torch.Size([2, 77])\n",
      "텍스트 토큰 디바이스: cpu\n",
      "텍스트 임베딩 크기: torch.Size([2, 512])\n",
      "텍스트 임베딩 디바이스: cpu\n",
      "임베딩 norm: tensor([1.0000, 1.0000])\n",
      "텍스트 간 코사인 유사도: 0.9800\n",
      "Temperature: 14.2857\n",
      "Temperature 적용된 유사도: 13.9997\n",
      "\n",
      "텍스트 간 유사도 매트릭스 (코사인 유사도):\n",
      "a photo of a ca vs a photo of a ca: 1.0000\n",
      "a photo of a ca vs a photo of a do: 0.9800\n",
      "a photo of a ca vs a photo of a ca: 0.9801\n",
      "a photo of a ca vs a photo of a tr: 0.9798\n",
      "a photo of a do vs a photo of a ca: 0.9800\n",
      "a photo of a do vs a photo of a do: 1.0000\n",
      "a photo of a do vs a photo of a ca: 0.9852\n",
      "a photo of a do vs a photo of a tr: 0.9849\n",
      "a photo of a ca vs a photo of a ca: 0.9801\n",
      "a photo of a ca vs a photo of a do: 0.9852\n",
      "a photo of a ca vs a photo of a ca: 1.0000\n",
      "a photo of a ca vs a photo of a tr: 0.9870\n",
      "a photo of a tr vs a photo of a ca: 0.9798\n",
      "a photo of a tr vs a photo of a do: 0.9849\n",
      "a photo of a tr vs a photo of a ca: 0.9870\n",
      "a photo of a tr vs a photo of a tr: 1.0000\n",
      "\n",
      "이미지 임베딩 크기: torch.Size([2, 512])\n",
      "이미지 임베딩 norm: tensor([1.0000, 1.0000])\n",
      "\n",
      "이미지-텍스트 유사도 매트릭스:\n",
      "tensor([[0.0116, 0.0236],\n",
      "        [0.0195, 0.0320]])\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로 간단한 테스트\n",
    "model.eval()\n",
    "\n",
    "# 디바이스 확인 및 설정\n",
    "device = next(model.parameters()).device\n",
    "print(f\"모델이 위치한 디바이스: {device}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 텍스트 임베딩 생성\n",
    "    texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "    text_tokens = model.model.text_encoder.tokenize(texts)\n",
    "    \n",
    "    # 토큰을 모델과 같은 디바이스로 이동\n",
    "    text_tokens = text_tokens.to(device)\n",
    "    print(f\"텍스트 토큰 shape: {text_tokens.shape}\")\n",
    "    print(f\"텍스트 토큰 디바이스: {text_tokens.device}\")\n",
    "    \n",
    "    # 정규화된 텍스트 임베딩 얻기\n",
    "    text_embeds = model.model.encode_text(text_tokens)\n",
    "    \n",
    "    print(f\"텍스트 임베딩 크기: {text_embeds.shape}\")\n",
    "    print(f\"텍스트 임베딩 디바이스: {text_embeds.device}\")\n",
    "    print(f\"임베딩 norm: {text_embeds.norm(dim=-1)}\")  # L2 정규화 확인 (1에 가까워야 함)\n",
    "    \n",
    "    # 코사인 유사도 계산 (정규화된 벡터 간의 내적)\n",
    "    cosine_similarity = (text_embeds[0] @ text_embeds[1]).item()\n",
    "    print(f\"텍스트 간 코사인 유사도: {cosine_similarity:.4f}\")\n",
    "    \n",
    "    # temperature가 적용된 유사도\n",
    "    temperature = model.model.temperature.item()\n",
    "    scaled_similarity = temperature * cosine_similarity\n",
    "    print(f\"Temperature: {temperature:.4f}\")\n",
    "    print(f\"Temperature 적용된 유사도: {scaled_similarity:.4f}\")\n",
    "    \n",
    "    # 추가 테스트: 더 많은 텍스트들\n",
    "    more_texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a tree\"]\n",
    "    more_tokens = model.model.text_encoder.tokenize(more_texts)\n",
    "    more_tokens = more_tokens.to(device)\n",
    "    \n",
    "    more_embeds = model.model.encode_text(more_tokens)\n",
    "    similarity_matrix = more_embeds @ more_embeds.t()\n",
    "    \n",
    "    print(f\"\\n텍스트 간 유사도 매트릭스 (코사인 유사도):\")\n",
    "    for i, text1 in enumerate(more_texts):\n",
    "        for j, text2 in enumerate(more_texts):\n",
    "            sim = similarity_matrix[i, j].item()\n",
    "            print(f\"{text1[:15]:15} vs {text2[:15]:15}: {sim:.4f}\")\n",
    "    \n",
    "    # 이미지 테스트 (간단한 더미 이미지)\n",
    "    dummy_images = torch.randn(2, 3, 224, 224).to(device)\n",
    "    image_embeds = model.model.encode_image(dummy_images)\n",
    "    print(f\"\\n이미지 임베딩 크기: {image_embeds.shape}\")\n",
    "    print(f\"이미지 임베딩 norm: {image_embeds.norm(dim=-1)}\")\n",
    "    \n",
    "    # 이미지-텍스트 간 유사도\n",
    "    img_text_sim = image_embeds @ text_embeds.t()\n",
    "    print(f\"\\n이미지-텍스트 유사도 매트릭스:\")\n",
    "    print(img_text_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
